{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF A JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PyPDF2\n",
    "# import re\n",
    "# import json\n",
    "\n",
    "\n",
    "# def extraer_info_desde_texto(texto):\n",
    "#     # Expresiones regulares para encontrar capítulos y artículos\n",
    "#     # regex_capitulo = re.compile(r'CAPÍTULO ([IVXLC]+): (.+)')\n",
    "#     regex_capitulo = re.compile(r'.*CAP[IÍ]TULO ([IVXLC]+):.*')\n",
    "#     # regex_articulo = re.compile(r'Art[ií]culo (\\d+)\\. – (.+)')\n",
    "#     # regex_articulo = re.compile(r'.*(Artículo).*')\n",
    "    \n",
    "#     info = []\n",
    "#     capitulo_actual = None\n",
    "#     articulo_actual = None\n",
    "#     texto_actual = ''\n",
    "\n",
    "    \n",
    "#     # Iterar sobre cada línea del texto\n",
    "#     for linea in texto.split('\\n'):\n",
    "#         # Buscar coincidencias con las expresiones regulares\n",
    "#         match_capitulo = regex_capitulo.match(linea)\n",
    "#         # match_articulo = regex_articulo.match(linea)\n",
    "\n",
    "#         if \"CAPITULO\" in linea:\n",
    "#             print(linea)\n",
    "\n",
    "#         # Si hay coincidencia con el formato de un capítulo\n",
    "#         if match_capitulo:\n",
    "#             if capitulo_actual and articulo_actual and texto_actual:\n",
    "#                 info.append({\n",
    "#                     'source': \"Estatutos (2022)\",\n",
    "#                     \"page\": 5,\n",
    "#                     'capitulo': capitulo_actual.strip(),\n",
    "#                     'articulo': articulo_actual.strip(),\n",
    "#                     'contenido': texto_actual.strip()\n",
    "#                 })\n",
    "#                 texto_actual = ''\n",
    "\n",
    "#             # Guardar el capítulo actual y reiniciar el artículo y el texto\n",
    "#             capitulo_actual = f\"CAPÍTULO {match_capitulo.group(1)}: \"\n",
    "#             articulo_actual = None\n",
    "#             texto_actual = ''\n",
    "#             print(\"CAPITULO:\", capitulo_actual)\n",
    "        \n",
    "#         # Si hay coincidencia con el formato de un artículo\n",
    "#         elif \"Artículo\" in linea:\n",
    "#             if capitulo_actual and articulo_actual and texto_actual:\n",
    "#                 info.append({\n",
    "#                     'source': \"Estatutos (2022)\",\n",
    "#                     \"page\": 5,\n",
    "#                     'capitulo': capitulo_actual.strip(),\n",
    "#                     'articulo': articulo_actual.strip(),\n",
    "#                     'contenido': texto_actual.strip()\n",
    "#                 })\n",
    "#                 texto_actual = ''\n",
    "\n",
    "#             # Guardar el artículo actual y reiniciar el texto\n",
    "#             # articulo_actual = match_articulo.group(1)\n",
    "#             articulo_actual = linea.strip()\n",
    "#             # texto_actual = match_articulo.group(2)\n",
    "#             texto_actual = ''\n",
    "#             print(\"ARTICULO:\", articulo_actual)\n",
    "        \n",
    "#         elif articulo_actual is None and capitulo_actual is not None:\n",
    "#             capitulo_actual += linea.strip() + ' '\n",
    "        \n",
    "#         # Si no hay coincidencia con ninguno de los formatos anteriores, añadir la línea al texto actual\n",
    "#         else:\n",
    "#             texto_actual += linea + ' '\n",
    "        \n",
    "#         # Si tenemos un capítulo y un artículo actual y un texto, añadirlos a la lista de información\n",
    "#         # if capitulo_actual and articulo_actual and texto_actual:\n",
    "#         #     info.append({\n",
    "#         #         'capitulo': capitulo_actual.strip(),\n",
    "#         #         'articulo': articulo_actual.strip(),\n",
    "#         #         'texto': texto_actual.strip()\n",
    "#         #     })\n",
    "#         #     texto_actual = ''\n",
    "    \n",
    "#     return info\n",
    "\n",
    "# def extraer_texto_desde_pdf(pdf_path):\n",
    "#     texto = ''\n",
    "#     with open(pdf_path, 'rb') as file:\n",
    "#         reader = PyPDF2.PdfReader(file)\n",
    "#         for page_num in range(4, len(reader.pages)):\n",
    "#             page = reader.pages[page_num]\n",
    "#             texto += page.extract_text()\n",
    "#     return texto\n",
    "\n",
    "# pdf_path = 'documentacion/1. Estatutos (2022).pdf'\n",
    "# save_path_result = \"documentacion/1. Estatutos (2022).json\"\n",
    "\n",
    "# # Extraer texto del PDF\n",
    "# texto_pdf = extraer_texto_desde_pdf(pdf_path)\n",
    "\n",
    "# # Extraer información del texto\n",
    "# informacion = extraer_info_desde_texto(texto_pdf)\n",
    "\n",
    "# # Convertir la información a formato JSON\n",
    "# json_info = json.dumps(informacion, indent=4, ensure_ascii=False)\n",
    "\n",
    "# with open(save_path_result, \"w\", encoding='utf-8') as json_file:\n",
    "#     json_file.write(json_info)\n",
    "# print(json_info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VECTOR_DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"documentacion/1. Estatutos (2022).json\"\n",
    "output_path = \"vector_db\"\n",
    "reset_db = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "\n",
    "\n",
    "class VectorDBGenerator:\n",
    "    def __init__(self, output_path, reset_db=True):\n",
    "        self.output_path = output_path\n",
    "        if reset_db:\n",
    "            self.__delete_db()\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "    def generate_vectordb(self, input_path):\n",
    "        json_data = self.__read_json_file(input_path)\n",
    "        documents = self.__generate_documents(json_data)\n",
    "        vectordb = self.__generate_vectors(documents)\n",
    "        return vectordb\n",
    "    \n",
    "\n",
    "    def __read_json_file(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "\n",
    "\n",
    "    def __generate_documents(self, data):\n",
    "        splits = []\n",
    "        for el in data:\n",
    "            metadata = {}\n",
    "            metadata[\"source\"] = el[\"source\"]\n",
    "            metadata[\"page\"] = el[\"page\"]\n",
    "            metadata[\"capitulo\"] = el[\"capitulo\"]\n",
    "            metadata[\"articulo\"] = el[\"articulo\"]\n",
    "            doc =  Document(page_content=el[\"contenido\"], metadata=metadata)\n",
    "            splits.append(doc)\n",
    "        return splits\n",
    "    \n",
    "    def __generate_vectors(self, documents):\n",
    "        vectordb = Chroma.from_documents(\n",
    "            documents=documents,\n",
    "            embedding=self.embeddings,\n",
    "            persist_directory=self.output_path\n",
    "        )\n",
    "        return vectordb\n",
    "    \n",
    "    def __delete_db(self):\n",
    "        if os.path.exists(self.output_path):\n",
    "            shutil.rmtree(self.output_path)\n",
    "        os.makedirs(self.output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = VectorDBGenerator(output_path, reset_db)\n",
    "generator.generate_vectordb(input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"../Llama-2-7b-chat-hf\"\n",
    "input_path = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load db\n",
    "vectordb = Chroma(persist_directory=input_path, embedding_function=HuggingFaceEmbeddings())\n",
    "print(vectordb._collection.count())\n",
    "\n",
    "\n",
    "# Load model directly\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "model.to(device)\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "llm=HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Build prompt\n",
    "template = \"\"\"Utiliza los siguientes elementos de contexto para responder a la pregunta del final. Genera una única respuesta. Si no sabes la respuesta, di simplemente que no la sabes, no intentes inventarte una respuesta. Utiliza tres frases como máximo. La respuesta debe ser lo más concisa posible.\n",
    "{context}\n",
    "Pregunta: {question}\n",
    "Respuesta:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "# Run chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imprimir_respuesta(result):\n",
    "    aswer = result[\"result\"]\n",
    "    print(f\"PREGUNTA: {result['query']}\")\n",
    "    print(f\"RESPUESTA: {aswer}\")\n",
    "    print(\"FUENTES:\")\n",
    "    for el in result[\"source_documents\"]:\n",
    "        print(\"{\" + f\"'source': '{el.metadata['source']}', 'page': '{el.metadata['page']}', 'capitulo': '{el.metadata['capitulo']}', 'articulo': '{el.metadata['articulo']}\" + \"}\")\n",
    "        print(el.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Cómo se puede disolver el grupo? Dime los pasos y condiciones\"\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "\n",
    "imprimir_respuesta(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"¿Quien es el Kraal?\"\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "\n",
    "imprimir_respuesta(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
