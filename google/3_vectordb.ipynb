{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Llama-2-7b-chat-hf\"\n",
    "# input_path = \"pipeline_files/2_transcribed_audio\"\n",
    "input_path = \"pipeline_files/2_transcribed_audio/gestion_de_requisitos_con_Redmine.json\"\n",
    "output_path = \"pipeline_files/3_vectordb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "if os.path.exists(output_path):\n",
    "    shutil.rmtree(output_path)\n",
    "os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "# Example usage:\n",
    "json_data = read_json_file(input_path)\n",
    "print(json_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_duration = 10\n",
    "\n",
    "def time_to_seconds(time_str):\n",
    "    time_components = time_str.split(':')\n",
    "    hours = int(time_components[0])\n",
    "    minutes = int(time_components[1])\n",
    "    seconds, milliseconds = map(float, time_components[2].split(','))\n",
    "    total_seconds = hours * 3600 + minutes * 60 + seconds + milliseconds / 1000\n",
    "    return total_seconds\n",
    "\n",
    "duration_chunk, chunk_text, chunk_start, chunk_end = 0, \"\", \"00:00:00,000\", \"00:00:00,000\"\n",
    "chunks = []\n",
    "for el in json_data:\n",
    "    start = float(time_to_seconds(el[\"start\"]))\n",
    "    end = float(time_to_seconds(el[\"end\"]))\n",
    "    duration_chunk += end - start\n",
    "    chunk_text += el[\"text\"] + \" \"\n",
    "    if duration_chunk >= chunk_duration:\n",
    "        chunk_end = el[\"end\"]\n",
    "        chunks.append({\"filename\": el[\"filename\"], \"start\": chunk_start, \"end\": chunk_end, \"text\": chunk_text})\n",
    "        chunk_start = el[\"end\"]\n",
    "        chunk_text = \"\"\n",
    "        duration_chunk = 0\n",
    "print(chunks)\n",
    "json_data = chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "splits = []\n",
    "for el in json_data:\n",
    "    metadata = {}\n",
    "    metadata[\"filename\"] = el[\"filename\"]\n",
    "    metadata[\"start\"] = el[\"start\"]\n",
    "    metadata[\"end\"] = el[\"end\"]\n",
    "    doc =  Document(page_content=el[\"text\"], metadata=metadata)\n",
    "    splits.append(doc)\n",
    "splits[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VECTORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=HuggingFaceEmbeddings(),\n",
    "    persist_directory=output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"que es la wiki?\"\n",
    "answer = vectordb.similarity_search(question,k=3)\n",
    "for el in answer:\n",
    "    print(\"{\" + f\"'filename': '{el.metadata['filename']}', 'start': '{el.metadata['start']}', 'end': '{el.metadata['end']}'\" + \"}\")\n",
    "    print(el.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_path = \"pipeline_files/2_transcribed_audio/gestion_de_requisitos_con_Redmine.json\"\n",
    "input_path = \"pipeline_files/2_transcribed_audio_coffee\"\n",
    "# output_path = \"pipeline_files/3_vectordb\"\n",
    "output_path = \"pipeline_files/3_vectordb_coffee\"\n",
    "chunk_duration = 10 # in seconds\n",
    "reset_db = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hugo\\git\\AutoClassNotes\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "\n",
    "\n",
    "class VectorDBGenerator:\n",
    "    def __init__(self, output_path, reset_db=True):\n",
    "        self.output_path = output_path\n",
    "        if reset_db:\n",
    "            self.__delete_db()\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "    def generate_vectordb(self, input_path, chunk_duration):\n",
    "        json_data = self.__read_json_file(input_path)\n",
    "        chunks = self.__chunk_aggregator(json_data, chunk_duration)\n",
    "        documents = self.__generate_documents(chunks)\n",
    "        vectordb = self.__generate_vectors(documents)\n",
    "        return vectordb\n",
    "    \n",
    "\n",
    "    def __read_json_file(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "\n",
    "\n",
    "    def __chunk_aggregator(self, data, chunk_duration):\n",
    "        # duration_chunk, chunk_text, chunk_start, chunk_end = 0, \"\", \"00:00:00,000\", \"00:00:00,000\"\n",
    "        duration_chunk, chunk_text, chunk_start, chunk_end = 0, \"\", \"00:00:00.000\", \"00:00:00.000\"\n",
    "        chunks = []\n",
    "        for el in data:\n",
    "            start = float(self.__time_to_seconds(el[\"start\"]))\n",
    "            end = float(self.__time_to_seconds(el[\"end\"]))\n",
    "            duration_chunk += end - start\n",
    "            # chunk_text += el[\"text\"] + \" \"\n",
    "            chunk_text += el[\"content\"] + \" \"\n",
    "            if duration_chunk >= chunk_duration:\n",
    "                chunk_end = el[\"end\"]\n",
    "                # chunks.append({\"filename\": el[\"filename\"], \"start\": chunk_start, \"end\": chunk_end, \"text\": chunk_text})\n",
    "                chunks.append({\"episode\": el[\"episode\"], \"start\": chunk_start, \"end\": chunk_end, \"content\": chunk_text})\n",
    "                chunk_start = el[\"end\"]\n",
    "                chunk_text = \"\"\n",
    "                duration_chunk = 0\n",
    "        return chunks\n",
    "    \n",
    "    def __time_to_seconds(self, time_str):\n",
    "        time_components = time_str.split(':')\n",
    "        hours = int(time_components[0])\n",
    "        minutes = int(time_components[1])\n",
    "        # seconds, milliseconds = map(float, time_components[2].split(','))\n",
    "        seconds, milliseconds = map(float, time_components[2].split('.'))\n",
    "        total_seconds = hours * 3600 + minutes * 60 + seconds + milliseconds / 1000\n",
    "        return total_seconds\n",
    "    \n",
    "\n",
    "    def __generate_documents(self, data):\n",
    "        splits = []\n",
    "        for el in data:\n",
    "            metadata = {}\n",
    "            # metadata[\"filename\"] = el[\"filename\"]\n",
    "            # metadata[\"start\"] = el[\"start\"]\n",
    "            # metadata[\"end\"] = el[\"end\"]\n",
    "            # doc =  Document(page_content=el[\"text\"], metadata=metadata)\n",
    "            metadata[\"filename\"] = el[\"episode\"]\n",
    "            metadata[\"start\"] = el[\"start\"]\n",
    "            metadata[\"end\"] = el[\"end\"]\n",
    "            doc =  Document(page_content=el[\"content\"], metadata=metadata)\n",
    "            splits.append(doc)\n",
    "        return splits\n",
    "    \n",
    "    def __generate_vectors(self, documents):\n",
    "        vectordb = Chroma.from_documents(\n",
    "            documents=documents,\n",
    "            embedding=self.embeddings,\n",
    "            persist_directory=self.output_path\n",
    "        )\n",
    "        return vectordb\n",
    "    \n",
    "    def __delete_db(self):\n",
    "        if os.path.exists(self.output_path):\n",
    "            shutil.rmtree(self.output_path)\n",
    "        os.makedirs(self.output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator = VectorDBGenerator(output_path, reset_db)\n",
    "# if os.path.isdir(input_path):\n",
    "#     for file_name in os.listdir(input_path):\n",
    "#         if file_name.endswith(\".json\"):\n",
    "#             file_path = os.path.join(input_path, file_name)\n",
    "#             generator.generate_vectordb(file_path, chunk_duration)\n",
    "# else:\n",
    "#     generator.generate_vectordb(input_path, chunk_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 490/490 [04:39<00:00,  1.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "generator = VectorDBGenerator(output_path, reset_db)\n",
    "if os.path.isdir(input_path):\n",
    "    for file_name in tqdm(os.listdir(input_path), desc=\"Processing files\"):\n",
    "        if file_name.endswith(\".json\"):\n",
    "            file_path = os.path.join(input_path, file_name)\n",
    "            generator.generate_vectordb(file_path, chunk_duration)\n",
    "else:\n",
    "    generator.generate_vectordb(input_path, chunk_duration)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
