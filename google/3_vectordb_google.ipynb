{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_path = \"documentacion/1. Estatutos (2022).md\"\n",
    "input_path = \"documentacion/2. Programa Educativo de Grupo 2022 - 2025.md\"\n",
    "# input_path = \"documentacion/3. Reglamento de Regimen Interno (2023).md\"\n",
    "# input_path = \"documentacion/4. Normas de Organización y Funcionamiento (2023).md\"\n",
    "output_path = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import openai\n",
    "# import sys\n",
    "# sys.path.append('../..')\n",
    "\n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "# openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MARKDOWN LOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_md_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            md_content = file.read()\n",
    "        return md_content\n",
    "    except FileNotFoundError:\n",
    "        print(\"El archivo no fue encontrado.\")\n",
    "        return None\n",
    "\n",
    "# Ejemplo de uso\n",
    "md_string = read_md_file(input_path)\n",
    "md_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "\n",
    "# Fichero 1\n",
    "# headers_to_split_on = [\n",
    "#     (\"#\", \"source\"),\n",
    "#     (\"##\", \"capitulo\"),\n",
    "#     (\"###\", \"articulo\"),\n",
    "#     (\"####\", \"articulo\"),\n",
    "# ]\n",
    "\n",
    "# Fichero 2\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"source\"),\n",
    "    (\"##\", \"capitulo\"),\n",
    "    (\"###\", \"apartado\"),\n",
    "    (\"####\", \"subapartado\"),\n",
    "]\n",
    "\n",
    "# Ficheros 3 y 4\n",
    "# headers_to_split_on = [\n",
    "#     (\"#\", \"source\"),\n",
    "#     (\"##\", \"capitulo\"),\n",
    "#     (\"###\", \"seccion\"),\n",
    "#     (\"####\", \"articulo\"),\n",
    "# ]\n",
    "\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on\n",
    ")\n",
    "splits = markdown_splitter.split_text(md_string)\n",
    "for doc in splits:\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VECTORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENAI_API_KEY = \"aqui tu key de openai\"\n",
    "PROJECT_ID = 'chatgsa-415807'\n",
    "REGION = \"europe-southwest1\"\n",
    "LLM_MODEL_NAME = \"gemini-1.0-pro\" # \"text-bison@001\"\n",
    "EMBEDDINGS_MODEL_NAME  = 'textembedding-gecko@001'\n",
    "BUCKET_NAME = \"gs://chatgsa-bucket\"\n",
    "INDEX_ID = '8496040697034440704'\n",
    "ENDPOINT_ID = '7291327796712833024'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.cloud import aiplatform\n",
    "# from langchain_community.vectorstores import MatchingEngine\n",
    "\n",
    "# new_index_name = \"chatgsa\"\n",
    "\n",
    "# aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "# print(f\"Creating index {new_index_name}...\")\n",
    "# index = aiplatform.MatchingEngineIndex.create_brute_force_index(\n",
    "#     display_name=f'{new_index_name}-index',\n",
    "#     contents_delta_uri=BUCKET_NAME,\n",
    "#     dimensions=768,\n",
    "#     sync=True,\n",
    "# )\n",
    "\n",
    "# print(f\"Creating index endpoint {new_index_name}...\")\n",
    "# index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
    "#     display_name = f\"{new_index_name}-endpoint\",\n",
    "#     public_endpoint_enabled = True\n",
    "# )\n",
    "\n",
    "# # print(f\"Deploying index {new_index_name}...\")\n",
    "# # deployed_index_name = new_index_name.replace('-', '_') # Para este parámetro, el nombre no puede incluir '-', pero sí '_'\n",
    "# # index_endpoint.deploy_index(\n",
    "# #     index=index, deployed_index_id=f'{deployed_index_name}_deployed_index'\n",
    "# # )\n",
    "\n",
    "# print(index)\n",
    "# print(index_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.vertexai import VertexAIEmbeddings\n",
    "\n",
    "embeddings = VertexAIEmbeddings(model_name=EMBEDDINGS_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(vectordb)\n",
    "    print(\"BD ya instanciada\")\n",
    "except:\n",
    "    print(\"Instanciando BD\")\n",
    "    vectordb = MatchingEngine.from_components(\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        gcs_bucket_name=BUCKET_NAME,\n",
    "        embedding=embeddings,\n",
    "        index_id=INDEX_ID,\n",
    "        endpoint_id=ENDPOINT_ID,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.add_documents(documents=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectordb = Chroma.from_documents(\n",
    "#     documents=splits,\n",
    "#     embedding=embedding,\n",
    "#     persist_directory=output_path\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # input_path = \"pipeline_files/2_transcribed_audio/gestion_de_requisitos_con_Redmine.json\"\n",
    "# input_path = \"pipeline_files/2_transcribed_audio_coffee\"\n",
    "# # output_path = \"pipeline_files/3_vectordb\"\n",
    "# output_path = \"pipeline_files/3_vectordb_coffee\"\n",
    "# chunk_duration = 10 # in seconds\n",
    "# reset_db = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# from langchain.docstore.document import Document\n",
    "# from langchain.vectorstores import Chroma\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# import os\n",
    "# import shutil\n",
    "# import torch\n",
    "\n",
    "\n",
    "# class VectorDBGenerator:\n",
    "#     def __init__(self, output_path, reset_db=True):\n",
    "#         self.output_path = output_path\n",
    "#         if reset_db:\n",
    "#             self.__delete_db()\n",
    "#         self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#         self.embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "#     def generate_vectordb(self, input_path, chunk_duration):\n",
    "#         json_data = self.__read_json_file(input_path)\n",
    "#         chunks = self.__chunk_aggregator(json_data, chunk_duration)\n",
    "#         documents = self.__generate_documents(chunks)\n",
    "#         vectordb = self.__generate_vectors(documents)\n",
    "#         return vectordb\n",
    "    \n",
    "\n",
    "#     def __read_json_file(self, file_path):\n",
    "#         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#             data = json.load(file)\n",
    "#         return data\n",
    "\n",
    "\n",
    "#     def __chunk_aggregator(self, data, chunk_duration):\n",
    "#         # duration_chunk, chunk_text, chunk_start, chunk_end = 0, \"\", \"00:00:00,000\", \"00:00:00,000\"\n",
    "#         duration_chunk, chunk_text, chunk_start, chunk_end = 0, \"\", \"00:00:00.000\", \"00:00:00.000\"\n",
    "#         chunks = []\n",
    "#         for el in data:\n",
    "#             start = float(self.__time_to_seconds(el[\"start\"]))\n",
    "#             end = float(self.__time_to_seconds(el[\"end\"]))\n",
    "#             duration_chunk += end - start\n",
    "#             # chunk_text += el[\"text\"] + \" \"\n",
    "#             chunk_text += el[\"content\"] + \" \"\n",
    "#             if duration_chunk >= chunk_duration:\n",
    "#                 chunk_end = el[\"end\"]\n",
    "#                 # chunks.append({\"filename\": el[\"filename\"], \"start\": chunk_start, \"end\": chunk_end, \"text\": chunk_text})\n",
    "#                 chunks.append({\"episode\": el[\"episode\"], \"start\": chunk_start, \"end\": chunk_end, \"content\": chunk_text})\n",
    "#                 chunk_start = el[\"end\"]\n",
    "#                 chunk_text = \"\"\n",
    "#                 duration_chunk = 0\n",
    "#         return chunks\n",
    "    \n",
    "#     def __time_to_seconds(self, time_str):\n",
    "#         time_components = time_str.split(':')\n",
    "#         hours = int(time_components[0])\n",
    "#         minutes = int(time_components[1])\n",
    "#         # seconds, milliseconds = map(float, time_components[2].split(','))\n",
    "#         seconds, milliseconds = map(float, time_components[2].split('.'))\n",
    "#         total_seconds = hours * 3600 + minutes * 60 + seconds + milliseconds / 1000\n",
    "#         return total_seconds\n",
    "    \n",
    "\n",
    "#     def __generate_documents(self, data):\n",
    "#         splits = []\n",
    "#         for el in data:\n",
    "#             metadata = {}\n",
    "#             # metadata[\"filename\"] = el[\"filename\"]\n",
    "#             # metadata[\"start\"] = el[\"start\"]\n",
    "#             # metadata[\"end\"] = el[\"end\"]\n",
    "#             # doc =  Document(page_content=el[\"text\"], metadata=metadata)\n",
    "#             metadata[\"filename\"] = el[\"episode\"]\n",
    "#             metadata[\"start\"] = el[\"start\"]\n",
    "#             metadata[\"end\"] = el[\"end\"]\n",
    "#             doc =  Document(page_content=el[\"content\"], metadata=metadata)\n",
    "#             splits.append(doc)\n",
    "#         return splits\n",
    "    \n",
    "#     def __generate_vectors(self, documents):\n",
    "#         vectordb = Chroma.from_documents(\n",
    "#             documents=documents,\n",
    "#             embedding=self.embeddings,\n",
    "#             persist_directory=self.output_path\n",
    "#         )\n",
    "#         return vectordb\n",
    "    \n",
    "#     def __delete_db(self):\n",
    "#         if os.path.exists(self.output_path):\n",
    "#             shutil.rmtree(self.output_path)\n",
    "#         os.makedirs(self.output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator = VectorDBGenerator(output_path, reset_db)\n",
    "# if os.path.isdir(input_path):\n",
    "#     for file_name in os.listdir(input_path):\n",
    "#         if file_name.endswith(\".json\"):\n",
    "#             file_path = os.path.join(input_path, file_name)\n",
    "#             generator.generate_vectordb(file_path, chunk_duration)\n",
    "# else:\n",
    "#     generator.generate_vectordb(input_path, chunk_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# generator = VectorDBGenerator(output_path, reset_db)\n",
    "# if os.path.isdir(input_path):\n",
    "#     for file_name in tqdm(os.listdir(input_path), desc=\"Processing files\"):\n",
    "#         if file_name.endswith(\".json\"):\n",
    "#             file_path = os.path.join(input_path, file_name)\n",
    "#             generator.generate_vectordb(file_path, chunk_duration)\n",
    "# else:\n",
    "#     generator.generate_vectordb(input_path, chunk_duration)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
